\documentclass{article}
\usepackage{amsmath}
\usepackage{booktabs}

\newcommand{\osn}{\oldstylenums}
\begin{document}

<<echo=F>>=
source('plot_results.R')
@
\title{Sequence Tagging: Part Two}
\author{Thomas Levine and Alec Story\\\small{tkl\osn{22} \& avs\osn{38}}}

\maketitle

%1. The Sequence Tagging Approach. Make clear which sequence
%tagging method(s) that you selected. Make clear which parts were im-
%plemented from scratch vs. obtained via an existing package. Explain
%and motivate any preprocessing and design choices.
\section{The Sequence Tagging Approach}
%which data set(s) and algorithm(s) will you include? will you implement the
We developed a part-of-speech tagger.
We implemented our tagger in Haskell, using memoization instead of the
traditional table-based Viterbi implementation (They are equivalent, but
memoization is more natural for functional programming languages like Haskell).

We broke the input into sentences, fed each of those into a sentence chunker,
and derived tag n-grams and lexical probabilities for the tags from these
sentences.  We chose sentences because that made it easier to avoid sentence
boundaries interfering with our results with long n-grams, and because the
implementation was easy in Haskell.

We implemented unknown word handling as an option.\footnote{We did not implement
command-line flags, so to change this, one must edit \texttt{Probabilities.hs}. }
With unknowns word handling on, the first time we see a word in the training set, we transform
it to \verb+<UNK>+.  When we look up the lexical probability of the word given a
tag, if there are no instances of that word with that tag, we return instead the
probability of \verb+<UNK>+ given that tag.  With unknowns off, we return 1 if
the tag is ``NNP'' and 0 otherwise.  We have shipped the code with a default of
bigrams, smoothing and no unknown handling.

We also implemented add-one smoothing, which is similarly controlled by a single
Boolean flag.

We should note that Haskell's implementation of Map, the hash-like data
structure we rely on heavily for looking up n-grams and lexical probabilities,
is implemented using trees, so insertion and lookup operations on it have a
runtime of $O(log\text{ } n)$.  This slows down our implementation, but
$log\text{ } n$ grows so slowly that in practice this is not likely to be a
serious detriment.  It should also be noted that all but the sneakiest
implementations of hashes have amortized $log\text{ } n$  insertion time, so
other languages also have this limitation for building their hashes, although
not for accessing them.

Otherwise, because of the memoization we use, our implementation should have the
same running time as other Viterbi implementations.  We observed that tagging
and printing an average sentence takes about half a second.

We also decided to use Haskell's built-in arbitrary precision arithmetic%
\footnote{Arbitrary precision arithmetic
deals with probabilities as ratios of integers, which are unbounded in size} to
avoid the vagaries of floating point errors.  Because this is tightly integrated
into Haskell, the implementation was actually simplified from what it would have
taken to write it using logarithmic arithmetic,\footnote{Our previous project
found nearly identical results between these two methods
for n-gram models of language, and the methods should be similarly close here}
but it slows down the code considerably.  We consider
this an acceptable trade-off.

%2. Experiments. Motivate and describe the experiments that you ran.
%What were your hypotheses as to what would happen?
\section{Experiments}
Initially, we planned on testing the effects of different system components
on the score. We decided to vary n-gram length, unknown word handling and
n-gram smoothing to see how these three components affected performance.

As we were developing the system, we noticed that some components performed
very poorly with small training sets. Specifically, we found our approach to
unknown word handling to work quite miserably on small test sets. Thus,
we also studied interactions between training set size and system components.

We used a $2\times2\times2$ factorial design to test the effects of the different
system components and their interactions on different system components.
The three factors in this design were
\begin{enumerate}
\item n-gram length (bigram or trigram)
\item n-gram smoothing (on or off)
\item unknown word handling (on or off)
\end{enumerate}
For two of the eight treatment combinations in this design, we also tested
different training corpus sizes. We chose two treatment combinations that were
identical except for the handling of unknown words so that we could test
for an interaction between unknown word handling and training corpus size;
for this sub-experiment held n-gram length constant (bigram) and n-gram
smoothing constant (on) while varying unknown word handling (on or off)
and corpus size. For this sub-experiment, we used two training corpus sizes
\begin{itemize}
\item 100,000 words
\item 996,759 words (full corpus)
\end{itemize}
This sub-experiment was thus a $2\times2$ factorial experiment run for
bigrams with bigram smoothing on.

We made the following hypotheses.
\begin{enumerate}
\item The best performing treatment combination will be n-gram length 3,
n-gram smoothing on, unknown word handling off and the full corpus size.
\item Unknown word handling will generally decrease performance
\item A smaller training corpus will result in a lower score.
\item Unknown word handling will decrease performance more when the
training corpus is small than when the training corpus is large
\end{enumerate}

\begin{table}
\begin{tabular}{lcccc}
%\toprule
& \multicolumn{4}{c}{Factors} \\
\cmidrule{2-5}
       & Training size & $n$-gram length & $n$-gram smoothing & UNK\\
\cmidrule{2-5}
Levels &  10000 words  &  Bigram  & None    & None \\
       & 996759 words  &  Trigram & Add One & First occurrence  \\
\cmidrule{2-5}
%\bottomrule
\end{tabular}
\caption{\label{tab:ind_vars}Independent variables for experiments on the final system with a Hidden Markov Model}
\end{table}

%The document should describe the baseline system, report the baseline
%predictions for the task, and explain how you got them(e.g. wrote your
%own code, used a package, used the web-based scorer).
\subsection{Baseline system}
Our baseline system includes n-gram extraction,
but we do not use it.
It does not include the hidden Markov model either.

Instead of using Bayes' rule and using n-grams and hidden Markov models,
the system assumes that a word's part of speech depends only
on the word and not on its context. With this assumption,
The system computes probabilities for each tag given the word by counting the
number of times each (tag, word) pair appears, and for each word, emitting the
most likely tag.

Unknown words are tagged as proper noun.
The baseline system does not do any smoothing.

The predictions from the baseline system had a score of 93\%
from the prediction submission form.

%3. Results. Summarize the performance of your system on both the
%training and test data. Minimally, you should compare your results
%to the baseline. Please put the results into clearly labeled tables or
%diagrams and include a written summary of the results.


\begin{figure}
\begin{center}
<<echo=F,fig=T>>=
posPlot.n()
@
\end{center}
\caption{\label{fig:n}Score of predictions as a function of $n$-gram length, use of smoothing and use of unknown word handling with training on the full training corpus. ``BL'' refers to the baseline score.}
\end{figure}

\begin{figure}
\begin{center}
<<echo=F,fig=T>>=
posPlot.l()
@
\end{center}
\caption{\label{fig:l}Score of predictions as a function of training corpus size and use of unknown word handling with smoothed bigram probabilities. ``BL'' refers to the baseline score.}
\end{figure}


\section{Results \& Discussion}
None of our system adjustments yielded improved performance over the baseline.

\subsection{Best performance}
Aside from the baseline,
the best performing system was combination bigrams with $n$-gram smoothing on,
unknown word handling off and the full corpus size. With $n$-gram smoothing on
and unknown word handling off, $n$-gram length did not seem to matter much;
we found similar results for
bigrams (\Sexpr{subset(results,unk=='Off'&smooth.gram=='On'&size==996759&n==2)$score}\%)
and trigrams (\Sexpr{subset(results,unk=='Off'&smooth.gram=='On'&size==996759&n==3)$score}\%).

This performance was less good than that of the baseline, which simply chose the
most likely tag, defaulting to ``NNP.''  This indicates that simple heuristics
do very well, but we suspect that, with another factor of 10 increase to the
training data, we could probably do better than the baseline.

The baseline only scales with more training data in that it knows about more
words.  The hidden Markov model learns the same relationships, but also learns
the n-gram tag model, which gets better and better as input size increases,
while lexical probabilities stop improving once you've seen most of the words in
English.

\subsection{$n$-gram length}
There was not a consistent effect of $n$-gram length on performance.
With smoothing on and unknown word handling off, performance was
similar for bigrams and trigrams. With unknown word handling on,
bigrams performed better than trigrams. With smoothing off
but unknown word handling on, performance was better with trigrams.



\subsection{Unknown words}
Unknown word handling generally decreased performance. Whenever we turned
unknown word handling on while keeping everything else the same, performance
decreased substantially.  This is probably because, with our input set size,
there were many word tokens that occurred only once; transforming them
to unknown word tokens means that we don't see them at all, which reduces
the ability of our system to produce accurate results.

%with a mean decrease of
%subset(results,size==996759&unk=='On')-subset(results,size==996759&unk=='Off')

\subsection{Smoothing $\times$ unknown words}
When smoothing was turned on, turning unknown word handling on or off had
nearly no effect on performance for either of the $n$-gram lengths that we
tested (table \ref{tab:res}).

This may have occurred because smoothing was used
relatively infrequently when unknown word handling was on; when smoothing
was off, the only lexemes that would return probabilities of zero were
those for which no lexemes of that part of speech had been tagged as
unknown during the training. We may have observed more of a difference
between these two system combinations if we had used a smaller
training corpus such that there would be more parts of speech with
unknown words.

\subsection{Corpus size}
Increasing the training corpus size invariably yielded result in a higher score.
The increase in score was on the order of 10 points for every 1000\% increase
in training corpus size.  Increasing the training corpus improves the number of
words we've seen, which means that we have to guess a proper noun less often,
and also improves the n-gram model of tags, so we get better tag sequences in
the resulting tagging.

\subsection{Unknown words $\times$ corpus size}
There was a mild interaction between unknown word handling and corpus size. When
corpus size was varied, the difference in score was slightly larger when unknown
word handling was on than when it was off.  One can see this graphically in
figure \ref{fig:l}; the slopes of the two lines are different.  Note, however,
that this interaction effect is quite small compared to the main effect of
unknown word handling being turned on or off.  This is probably because
transforming the first occurrence of a word into an unk amplifies the effect of
having a strong, deep training corpus.  There are more words proportionally that
appear only once in shorter training corpora than in very long corpora that
contain many more words than English does.

\subsection{Limitations of the study}
There are many variables that we did not explore in our study
that could have improved performance.  For example, we did not determine the
effects of reducing all the input to lower case. Also, we could have written
a series of heuristics for choosing tags for words that we have not seen
instead of using our simpler method of handling unknown words.
Such techniques would likely have pushed performance higher, above that of the
baseline.

\begin{table}
\centering
\label{tab:mytable}
<<mytable,echo=F,results=tex>>=
       library(xtable)
       results_=results[c('n','unk','smooth.gram','size','score')][order(results$n),]
       names(results_)=c('n','UNK','Smoothing','Size','Score')
       row.names(results_)=1:11
       mat <- xtable(results_)
       print(mat,
             sanitize.text.function = function(x){x},
             floating=FALSE,
             hline.after=NULL)
@
\caption{\label{tab:res}All of our non-baseline results are presented in this table.
``n'' is the $n$-gram length, where 2 is bigram and 3 is trigram.
``Smoothing'' is whether $n$-gram smoothing was on or off.
``UNK'' is whether unknown word handling was on or off.
``Size'' refers to the number of words in the training corpus.
``Score'' is the percentage of words correctly tagged.
Our baseline score was 93\%.}
\end{table}

\section{Conclusion}
Our results demonstrate the importance of considering the interaction
of various system components in building an effective system. None of our
advanced features improved performance over the baseline. Also, some of
our system components interacted with other components such that a change
in performance caused by one new component was conditional on which
other components were being used.

\end{document}
