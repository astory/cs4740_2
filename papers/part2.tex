\documentclass{article}
\usepackage{booktabs}
%By 11:59pm, Thursday, March 10th, you need to submit:
%1. A short document that indicates your plan for the project
\newcommand{\osn}{\oldstylenums}
\title{Sequence Tagging: Part One}
\author{Thomas Levine and Alec Story\\\small{tkl\osn{22} \& avs\osn{38}}}
\begin{document}
\maketitle

%1. The Sequence Tagging Approach. Make clear which sequence
%tagging method(s) that you selected. Make clear which parts were im-
%plemented from scratch vs. obtained via an existing package. Explain
%and motivate any preprocessing and design choices.
\section{The Sequence Tagging Approach}

%which data set(s) and algorithm(s) will you include? will you implement the
\subsection{Data set}
We will study part-of-speech tagging.
\subsection{Algorithms}

We implemented our tagger in Haskell, using memoization instead of the
traditional table-based Viterbi implementation (they are equivalent, but
memoization is more natural for functional programming languages like Haskell).

We broke the input into sentences, fed each of those into a sentence chunker,
and derived tag n-grams and lexical probabilities for the tags from these
sentences.  We chose sentences because that made it easier to avoid sentence
boundaries interfering with our results with long n-grams, and because the
implementation was easy in Haskell.

We implemented unknown word handling as an option (although we did not implement
command-line flags, so to change this, one must edit \verb+Probabilities.hs+).
With unknowns on, the first time we see a word in the training set, we transform
it to \verb+<UNK>+.  When we look up the lexical probability of the word given a
tag, if there are no instances of that word with that tag, we return instead the
probability of \verb+<UNK>+ given that tag.  With unknowns off, we return 1 if
the tag is ``NNP'' and 0 otherwise.

We also implemented add-one smoothing, which is similarly controlled by a single
boolean flag.

We should note that Haskell's implementation of Map, the hash-like data
structure we rely on heavily for looking up n-grams and lexical probabilities is
implemented using trees, so insertion and lookup operations on it have a runtime
of $O(log\text{ } n)$.  This slows down our implementation, but $log\text{ } n$ grows so slowly
that in practice this is not likely to be a serious detriment.  It should also
be noted that all but the sneakiest implementations of hashes have amortized
$log\text{ } n$  insertion time.

Otherwise, because of the memoization we use, our implementation should have the
same running time as other Viterbi implementations.  We observed that tagging
and printing an average sentence takes about half a second.

%2. Experiments. Motivate and describe the experiments that you ran.
%What were your hypotheses as to what would happen?
\section{Experiments}
As we were developing the system, we noticed that some components performed
very poorly with small training sets. 
We studied interactions between training set size and system components.

\begin{table}
\begin{tabular}{lcccc}
%\toprule
& \multicolumn{4}{c}{Factors} \\
\cmidrule{2-5}
       & Training size & $n$-gram length & $n$-gram smoothing & UNK\\
\cmidrule{2-5}
Levels & 100     &  2  & None    & None \\
       & 1000    &  3  & Add One & First occurrance  \\
       & \vdots  &  \vdots  &     &      &      \\
       & 996759  &  5  &     &      &      \\
\cmidrule{2-5}
%\bottomrule
\end{tabular}
\caption{\label{tab:ind_vars}Independent variables for experiments on the final system with a Hidden Markov Model}
\end{table}

Our experiments will consider several of a variety of variables (table \ref{tab:ind_vars}): n-gram length,
n-gram smoothing (Good-Turing or none), lexical probability smoothing, unknown
word handling (counting the first occurrances of words as unknown) and
whether a hidden Markov model is used (comparison to the baseline).
We have not yet decided precisely which features we will implement, but we
aim to target all of them if time allows.

%The document should describe the baseline system, report the baseline
%predictions for the task, and explain how you got them(e.g. wrote your
%own code, used a package, used the web-based scorer).
\section{Baseline system}
Our baseline system includes n-gram extraction,
but we do not use it.
It does not include the hidden Markov model either.

Instead of using Bayes' rule and using n-grams and hidden Markov models,
the system assumes that a word's part of speech depends only
on the word and not on its context. With this assumption,
The system computes probabilities for each tag given the word by counting the
number of times each (tag, word) pair appears, and for each word, emitting the
most likely tag.

Unknown words are tagged as proper noun.
The baseline system does not do any smoothing.

The predictions from the baseline system had a score of 93\%
from the prediction submission form.

%3. Results. Summarize the performance of your system on both the
%training and test data. Minimally, you should compare your results
%to the baseline. Please put the results into clearly labeled tables or
%diagrams and include a written summary of the results.
\section{Results}


%4. Discussion of the results. This section should include any observa-
%tions regarding your system’s performance — e.g. When did it work
%well, when did it fail, why? How might you improve the system?
%Additional analysis is always good. You can do an error analysis —
%what sorts of errors occurred, and why? You can provide a learning
%curve — how does your model’s performance change with the amount
%of training data provided. You can come up with a better baseline and
%compare your systems against that.
\section{Discussion}

\end{document}
