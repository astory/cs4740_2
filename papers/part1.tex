\documentclass{article}
%By 11:59pm, Thursday, March 10th, you need to submit:
%1. A short document that indicates your plan for the project
\newcommand{\osn}{\oldstylenums}
\title{Sequence Tagging: Part One}
\author{Thomas Levine and Alec Story\\\small{tkl\osn{22} \& avs\osn{38}}}
\begin{document}
\maketitle
%which data set(s) and algorithm(s) will you include? will you implement the
\section{Data set}
We will study part-of-speech tagging.
\section{Algorithms}

We plan to implement all of our tools from scratch or nearly-scratch in Haskell,
a modern functional programming language with monadic I/O.  We have already
implemented a system to read the input files and produce a usable dictionary of
arbitrary n-grams (over arbitrary objects, in our case, we will use tags), and
can produce probabilities for any pair of arbitrary objects taken from a list
(in our case, words and tags).  These components yield all the data we need to
compute the Viterbi algorithm.  Our implementation of Viterbi will \emph{not} be
the traditional table-based implementation; rather, it will use memoization
because such an approach is more natural for a functional programming language
like Haskell.

Our experiments will consider several of a variety of variables: n-gram length,
n-gram smoothing (Good-Turing or none), lexical probability smoothing, unknown
word handling via at least one method and whether a hidden Markov model is used.
We have not yet decided precisely which features we will implement, but we
aim to target all of them if time allows.

%2. The document should describe the baseline system, report the baseline
%predictions for the task, and explain how you got them(e.g. wrote your
%own code, used a package, used the web-based scorer).
\section{Baseline system}
Our baseline system includes n-gram extraction,
but we do not use it.
It does not include the hidden Markov model either.

Instead of using Bayes' rule and using n-grams and hidden Markov models,
the system assumes that a word's part of speech depends only
on the word and not on its context. With this assumption,
The system computes probabilities for each tag given the word by counting the
number of times each (tag, word) pair appears, and for each word, emitting the
most likely tag.

Unknown words are tagged as proper noun.
The baseline system does not do any smoothing.

The predictions from the baseline system had a score of 47.83\%
from the prediction submission form.

%3. The code for the baseline system if you are implementing from
%scratch. If you are using a package, do not submit the code.

\section{The Code}

We recognize that Haskell is a language that may not be installed on your
machine.  Instructions for running our code with (and without!) Haskell
installed are included in our \verb+Readme+.
\end{document}
